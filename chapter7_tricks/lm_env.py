"""
PyTorch implementation of Language Model Environment.

There are two main components in this documentation:
- We use GPT-2 as the base language model and construct an environment.
- We make a demonstration of this environment and users can type prompts in the command line to interact with the language model.
"""
import torch
import gym
from typing import Callable, Optional, Dict, Tuple
# For more information about GPT2, please refer to this doc: <link https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html#gpt2lmheadmodel link>.
from transformers import GPT2Tokenizer, GPT2LMHeadModel


class TextHistory:
    """
    **Overview:**
        The TextHistory class keeps track of the history of an interaction between the language model and the environment.
    """

    def __init__(self, text: str, tokens: torch.Tensor):
        """
        Initialize TextHistory.
        Arguments:
            text: The text of the first segment.
            tokens: The tokens of the first segment.
        """
        # Record the total text generated by both user and language model.
        self.text = text
        # Recored the ranges of text for each reply.
        self.text_spans = []
        # Record the tokenized total text generated by both user and language model.
        self.tokens = tokens
        # This flag shows that wether this record is finished.
        self.completed = False

        self.append_segment(text, tokens)
    
    # delimiter
    def append_segment(self, text: str, tokens: torch.Tensor) -> None:
        """
        **Overview:**
            Append a new segment to the history.
        Arguments:
            text: The text of the new segment.
            tokens: The tokens of the new segment.
        """
        # If the text is empty, raise Error.
        if len(text) == 0 or len(tokens) == 0:
            raise ValueError("Can't append empty text or token list to history.")
        
        # Add the new text to ``self.text``
        original_text_length = len(self.text)
        self.text += text
        # Update the range of this new text segment.
        self.text_spans.append((original_text_length, len(self.text)))
        # Add the new tokens to ``self.tokens``.
        self.tokens = torch.cat((self.tokens, tokens))
    
    # delimiter
    @property
    def last_text_segment(self) -> str:
        """
        **Overview:**
            Get the last text segment.
        """
        start, end = self.text_spans[-1]
        return self.text[start:end]


# delimiter
class TextEnvironment(gym.Env):
    """
    **Overview:**
        The TextEnvironment enables interaction of a LLM with an environment.
    """

    def __init__(self, model: GPT2LMHeadModel, tokenizer: GPT2Tokenizer, reward_fn: Callable,
                 max_turns: int = 4, generation_kwargs: Optional[Dict] = None):
        """
        **Overview:**
            Initialize the TextEnvironment.

        Arguments:
            model: The model to use for generation.
            tokenizer: The tokenizer to use for generation.
            reward_fn: A callable function that takes a string and returns a reward.
            max_turns: The maximum number of turns to allow.
            generation_kwargs: A dictionary of keyword arguments to pass to the model's generate method.
        """
        # Initialize the arguments.
        self.model = model
        self.tokenizer = tokenizer
        self.reward_fn = reward_fn
        self.max_turns = max_turns
        
        # Prepare the arguments for text generation.
        if generation_kwargs is None:
            self.generation_kwargs = dict()
        else:
            self.generation_kwargs = generation_kwargs
        
        # Count the times of ``self.step()``
        self.turn = 0
        # Preserve the history of interactions.
        self.history = None
        # Determine the device of running the model (cpu or cuda).
        self.current_device = self.model.device
    
    # delimiter
    def reset(self):
        """
        **Overview:**
            Reset the environment.
        """
        # Reset the history and the counter of step number.
        self.history = None
        self.turn = 0
        return self.history
    
    # delimiter
    def generate(self) -> None:
        """
        **Overview:**
            Generate responses for a history.
        """
        # The input of model is all the interaction histories.
        query_tensors = self.history.tokens
        # Generate reply.
        response_tensors = self._generate(query_tensors)
        # Decode the reply into string format.
        response_texts = self.tokenizer.decode(response_tensors)
        # Add the new generated reply to ``self.history``
        self.history.append_segment(response_texts, response_tensors)

    # delimiter
    def step(self, query: str) -> Tuple[TextHistory, float, bool]:
        """
        **Overview:**
            The step function of the language model environment.
        """
        if self.history is None:
            query_tokens = self.tokenizer(query, return_tensors="pt").input_ids[0].to(self.current_device)
            self.history = TextHistory(query, query_tokens)
        else:
            query_tokens = self.tokenizer(query, return_tensors="pt").input_ids[0].to(self.current_device)
            self.history.append_segment(query, query_tokens)
        self.generate()
        rew = self.reward_fn(self.history.last_text_segment)
        self.turn += 1
        self.history.completed = self.turn >= self.max_turns
        return self.history, rew, self.history.completed

    # delimiter
    def _generate(self, query_tensors: torch.Tensor) -> torch.Tensor:
        """
        Generate responses for a list of query tensors.
        args:
            query_tensors (list[torch.Tensor]): A list of query tensors to generate responses for.
        """
        query_tensors = query_tensors.unsqueeze(0)
        batch_mask = torch.ones_like(query_tensors)
        inputs = {"input_ids": query_tensors, "attention_mask": batch_mask}

        generation = self.model.generate(**inputs, **self.generation_kwargs,
                                         pad_token_id=self.tokenizer.eos_token_id)

        output = generation[0, batch_mask[0].sum():]  # remove prompt
        return output


# delimiter
def test_env():
    """
    **Overview:**
        In this function, we test the language model environment and interact with it by typing prompts in the command line.
    """
    # Load the pretrained model and tokenizer.
    # When first call this function, the pretrained files will be automatically downloaded from <link https://huggingface.co/ link>.
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    # For simplicity, we set the reward function to return a constant value 1.
    reward_function = lambda x: 1
    # Arguments for text generation.
    generation_kwargs = {
        # The maximum number of tokens can be generated by language model is 20.
        'max_new_tokens': 20,
        # Use undeterministic method to sample generated results each time.
        'do_sample': True,
        # The temperature of softmax function for sampling.
        'temperature': 0.7,
        # Penalize the language model to generate repeated words.
        'repetition_penalty': 2.0
    }
    # Initialize the environment.
    env = TextEnvironment(model=model, tokenizer=tokenizer, max_turns=3, reward_fn=reward_function,
                          generation_kwargs=generation_kwargs)
    env.reset()
    
    # Interaction loop.
    while True:
        # User input the question.
        q = input("Please type in your question: ")
        # The env step once.
        his, reward, done = env.step(q)
        # Extract and print the response.
        print(f"Response: {his.last_text_segment}")
        # If the environment is done, break the interaction loop.
        if done:
            break


if __name__ == '__main__':
    test_env()
